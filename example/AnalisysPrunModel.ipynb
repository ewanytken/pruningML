{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22788536-5178-4287-89b4-2ff44ad38057",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from data import RationalWeight\n",
    "from data import Enumerator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224a5f26-95cf-4dac-ae87-af1c0f390609",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "data_files = {'train': 'train.parquet', 'test': 'test.parquet'}\n",
    "dataset = load_dataset('../datasets/imdbstfd', data_files=data_files)\n",
    "dataset = dataset.shuffle(seed=88)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"../models/bert-base-uncased-IMDB-retrain\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/bert-base-uncased-IMDB-retrain\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f9905e-c785-41a0-9b47-bad2f070c99c",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0dcee1e-a0dc-40e2-9e71-ab73ac260fe6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "201"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameExclude = [\"99\"]\n",
    "exclude = []\n",
    "for name, param in model.named_parameters():\n",
    "    for excludeName in nameExclude:\n",
    "        if excludeName in name:\n",
    "            exclude.append(name)\n",
    "\n",
    "layers = list(filter(lambda x: x not in exclude, [name for name, param in model.named_parameters()]))\n",
    "\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "N=6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbbd59d-5a5b-4420-9aa0-18a83c94c2a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'prunModel': <data.RationalWeightClass.RationalWeight at 0x13532d6a590>,\n  'data': {'sigma': 1,\n   'scale': 1.5,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.8976982097186701,\n    'F1': 0.8823529411764706,\n    'Precision': 0.8823529411764706,\n    'Recall': 0.8823529411764706},\n   'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.intermediate.dense.bias',\n    'bert.encoder.layer.1.attention.self.query.weight',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.2.intermediate.dense.bias',\n    'bert.encoder.layer.2.output.LayerNorm.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.3.intermediate.dense.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.output.dense.bias',\n    'bert.encoder.layer.4.output.dense.bias',\n    'bert.encoder.layer.5.attention.self.query.weight',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.value.bias',\n    'bert.encoder.layer.5.attention.output.dense.bias',\n    'bert.encoder.layer.6.attention.self.query.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.6.attention.self.value.bias',\n    'bert.encoder.layer.6.attention.output.dense.bias',\n    'bert.encoder.layer.6.intermediate.dense.bias',\n    'bert.encoder.layer.7.attention.self.query.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.7.output.LayerNorm.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.self.value.bias',\n    'bert.encoder.layer.8.attention.output.dense.bias',\n    'bert.encoder.layer.8.intermediate.dense.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.9.output.dense.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.self.value.bias',\n    'bert.encoder.layer.10.intermediate.dense.bias',\n    'bert.encoder.layer.10.output.LayerNorm.bias',\n    'bert.encoder.layer.11.attention.self.query.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.self.value.bias',\n    'bert.encoder.layer.11.attention.output.dense.bias',\n    'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.11.intermediate.dense.bias',\n    'bert.encoder.layer.11.output.dense.bias']}},\n {'prunModel': <data.RationalWeightClass.RationalWeight at 0x135253f3610>,\n  'data': {'sigma': 1,\n   'scale': 2,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.9437340153452685,\n    'F1': 0.8888888888888888,\n    'Precision': 0.8421052631578947,\n    'Recall': 0.9411764705882353},\n   'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.intermediate.dense.bias',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.1.attention.self.value.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.output.dense.bias',\n    'bert.encoder.layer.4.output.dense.bias',\n    'bert.encoder.layer.5.attention.self.query.weight',\n    'bert.encoder.layer.5.attention.self.query.bias',\n    'bert.encoder.layer.5.attention.self.key.weight',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.value.bias',\n    'bert.encoder.layer.5.attention.output.dense.bias',\n    'bert.encoder.layer.6.attention.self.query.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.self.query.weight',\n    'bert.encoder.layer.7.attention.self.query.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.self.value.bias',\n    'bert.encoder.layer.7.attention.output.dense.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.self.value.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.9.attention.self.value.bias',\n    'bert.encoder.layer.9.attention.output.dense.bias',\n    'bert.encoder.layer.9.intermediate.dense.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.self.value.bias',\n    'bert.encoder.layer.10.attention.output.dense.bias',\n    'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.11.attention.self.query.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.self.value.bias',\n    'bert.encoder.layer.11.attention.output.dense.bias',\n    'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.11.intermediate.dense.bias',\n    'bert.encoder.layer.11.output.dense.bias']}},\n {'prunModel': <data.RationalWeightClass.RationalWeight at 0x13529105550>,\n  'data': {'sigma': 2,\n   'scale': 1.5,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.9002557544757034,\n    'F1': 0.8823529411764706,\n    'Precision': 0.8823529411764706,\n    'Recall': 0.8823529411764706},\n   'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.intermediate.dense.bias',\n    'bert.encoder.layer.1.attention.self.query.bias',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.1.attention.self.value.bias',\n    'bert.encoder.layer.1.attention.output.dense.bias',\n    'bert.encoder.layer.1.output.dense.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.2.intermediate.dense.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.output.dense.bias',\n    'bert.encoder.layer.5.attention.self.query.bias',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.value.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.self.query.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.self.value.bias',\n    'bert.encoder.layer.9.attention.self.query.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.output.dense.bias',\n    'bert.encoder.layer.11.output.LayerNorm.bias',\n    'bert.pooler.dense.bias']}},\n {'prunModel': <data.RationalWeightClass.RationalWeight at 0x135291ef510>,\n  'data': {'sigma': 2,\n   'scale': 2,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.9053708439897699,\n    'F1': 0.8823529411764706,\n    'Precision': 0.8823529411764706,\n    'Recall': 0.8823529411764706},\n   'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.attention.self.value.bias',\n    'bert.encoder.layer.0.intermediate.dense.bias',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.1.attention.output.dense.bias',\n    'bert.encoder.layer.1.output.dense.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.value.bias',\n    'bert.encoder.layer.5.output.dense.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.6.attention.output.dense.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.9.attention.output.dense.bias',\n    'bert.encoder.layer.9.intermediate.dense.bias',\n    'bert.encoder.layer.9.output.dense.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.10.output.LayerNorm.bias',\n    'bert.encoder.layer.11.attention.self.query.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.self.value.bias',\n    'bert.encoder.layer.11.attention.output.dense.bias',\n    'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.11.intermediate.dense.bias',\n    'bert.encoder.layer.11.output.dense.weight',\n    'bert.encoder.layer.11.output.LayerNorm.bias',\n    'bert.pooler.dense.bias']}},\n {'prunModel': <data.RationalWeightClass.RationalWeight at 0x13533148e50>,\n  'data': {'sigma': 3,\n   'scale': 1.5,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.9258312020460358,\n    'F1': 0.8888888888888888,\n    'Precision': 0.8421052631578947,\n    'Recall': 0.9411764705882353},\n   'Pruning_layers': ['bert.embeddings.LayerNorm.bias',\n    'bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.attention.self.value.weight',\n    'bert.encoder.layer.0.attention.output.dense.weight',\n    'bert.encoder.layer.0.attention.output.dense.bias',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.1.attention.self.value.bias',\n    'bert.encoder.layer.1.intermediate.dense.bias',\n    'bert.encoder.layer.1.output.dense.bias',\n    'bert.encoder.layer.2.attention.self.query.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.2.intermediate.dense.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.self.value.bias',\n    'bert.encoder.layer.4.intermediate.dense.bias',\n    'bert.encoder.layer.4.output.dense.bias',\n    'bert.encoder.layer.5.attention.self.query.bias',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.intermediate.dense.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.6.attention.self.value.bias',\n    'bert.encoder.layer.7.attention.self.query.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.output.dense.bias',\n    'bert.encoder.layer.8.attention.self.query.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.8.intermediate.dense.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.output.dense.bias',\n    'bert.encoder.layer.10.intermediate.dense.bias',\n    'bert.encoder.layer.10.output.LayerNorm.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n    'bert.encoder.layer.11.output.dense.bias',\n    'bert.encoder.layer.11.output.LayerNorm.bias',\n    'bert.pooler.dense.bias']}},\n {'prunModel': <data.RationalWeightClass.RationalWeight at 0x135998d1f90>,\n  'data': {'sigma': 3,\n   'scale': 2,\n   'inside': True,\n   'RESULT': {'Accuracy': 0.9,\n    'ROC_AUC_score': 0.9053708439897699,\n    'F1': 0.8823529411764706,\n    'Precision': 0.8823529411764706,\n    'Recall': 0.8823529411764706},\n   'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias',\n    'bert.encoder.layer.0.intermediate.dense.bias',\n    'bert.encoder.layer.1.attention.self.key.bias',\n    'bert.encoder.layer.2.attention.self.key.bias',\n    'bert.encoder.layer.3.attention.self.key.bias',\n    'bert.encoder.layer.3.intermediate.dense.bias',\n    'bert.encoder.layer.4.attention.self.query.bias',\n    'bert.encoder.layer.4.attention.self.key.bias',\n    'bert.encoder.layer.4.attention.output.dense.bias',\n    'bert.encoder.layer.5.attention.self.query.bias',\n    'bert.encoder.layer.5.attention.self.key.bias',\n    'bert.encoder.layer.5.attention.self.value.bias',\n    'bert.encoder.layer.6.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.self.key.bias',\n    'bert.encoder.layer.7.attention.self.value.bias',\n    'bert.encoder.layer.8.attention.self.key.bias',\n    'bert.encoder.layer.8.attention.output.dense.bias',\n    'bert.encoder.layer.8.output.LayerNorm.bias',\n    'bert.encoder.layer.9.attention.self.key.bias',\n    'bert.encoder.layer.10.attention.self.key.bias',\n    'bert.encoder.layer.10.output.LayerNorm.bias',\n    'bert.encoder.layer.11.attention.self.query.bias',\n    'bert.encoder.layer.11.attention.self.key.bias',\n    'bert.encoder.layer.11.attention.self.value.bias']}}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PATH = \"../model-prun/models_3_3_095\"\n",
    "PATH = \"../model-prun/reprun-models\"\n",
    "\n",
    "# models = torch.load(PATH, map_location=torch.device('cpu'), weights_only=False)\n",
    "models = torch.load(PATH, weights_only=False)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57801e9-faf3-4603-8e16-35645e0a090a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prunLayers = [models[i]['data']['Pruning_layers'] for i in range(N)] #199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'Accuracy': 0.9,\n  'ROC_AUC_score': 0.8976982097186701,\n  'F1': 0.8823529411764706,\n  'Precision': 0.8823529411764706,\n  'Recall': 0.8823529411764706},\n {'Accuracy': 0.9,\n  'ROC_AUC_score': 0.9437340153452685,\n  'F1': 0.8888888888888888,\n  'Precision': 0.8421052631578947,\n  'Recall': 0.9411764705882353},\n {'Accuracy': 0.9,\n  'ROC_AUC_score': 0.9002557544757034,\n  'F1': 0.8823529411764706,\n  'Precision': 0.8823529411764706,\n  'Recall': 0.8823529411764706},\n {'Accuracy': 0.9,\n  'ROC_AUC_score': 0.9053708439897699,\n  'F1': 0.8823529411764706,\n  'Precision': 0.8823529411764706,\n  'Recall': 0.8823529411764706},\n {'Accuracy': 0.9,\n  'ROC_AUC_score': 0.9258312020460358,\n  'F1': 0.8888888888888888,\n  'Precision': 0.8421052631578947,\n  'Recall': 0.9411764705882353},\n {'Accuracy': 0.9,\n  'ROC_AUC_score': 0.9053708439897699,\n  'F1': 0.8823529411764706,\n  'Precision': 0.8823529411764706,\n  'Recall': 0.8823529411764706}]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[models[i]['data']['RESULT'] for i in range(N)] #199"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459520f0-6975-45a9-8b0b-ab863f1e6da3",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[42, 39, 27, 31, 38, 24]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(models[i]['data']['Pruning_layers']) for i in range(N)] #199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec059ae-104f-4391-aca8-0521aaa6d03f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perZerosInModel': tensor(0.1887, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20654641, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20240161, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20242962, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.2055, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(22496536, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.1919, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(21008161, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20243155, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    layers1_1 = models[i]['data']['Pruning_layers']\n",
    "    print(models[i]['prunModel'].zeroCounter(layers)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bda38d-1f07-4028-8770-806c9af8c023",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer': 'bert.encoder.layer.11.output.dense.bias', 'countZero': tensor(553, device='cuda:0'), 'percentZero': tensor(0.7201, device='cuda:0'), 'numParamLayer': 768}\n",
      "{'layer': 'bert.encoder.layer.11.output.dense.bias', 'countZero': tensor(553, device='cuda:0'), 'percentZero': tensor(0.7201, device='cuda:0'), 'numParamLayer': 768}\n",
      "{'layer': 'bert.pooler.dense.bias', 'countZero': tensor(704, device='cuda:0'), 'percentZero': tensor(0.9167, device='cuda:0'), 'numParamLayer': 768}\n",
      "{'layer': 'bert.pooler.dense.bias', 'countZero': tensor(704, device='cuda:0'), 'percentZero': tensor(0.9167, device='cuda:0'), 'numParamLayer': 768}\n",
      "{'layer': 'bert.pooler.dense.bias', 'countZero': tensor(766, device='cuda:0'), 'percentZero': tensor(0.9974, device='cuda:0'), 'numParamLayer': 768}\n",
      "{'layer': 'bert.encoder.layer.11.attention.self.value.bias', 'countZero': tensor(754, device='cuda:0'), 'percentZero': tensor(0.9818, device='cuda:0'), 'numParamLayer': 768}\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    layers1_1 = models[i]['data']['Pruning_layers']\n",
    "    print(models[i]['prunModel'].zeroCounter(layers1_1)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "551912ee-f9fb-4120-aa2a-bcca8008cb86",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sigma': 1, 'scale': 1.5, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.8976982097186701, 'F1': 0.8823529411764706, 'Precision': 0.8823529411764706, 'Recall': 0.8823529411764706}, 'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.bias']}\n",
      "{'perZerosInModel': tensor(0.1887, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20654641, device='cuda:0')}\n",
      "{'sigma': 1, 'scale': 2, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.9437340153452685, 'F1': 0.8888888888888888, 'Precision': 0.8421052631578947, 'Recall': 0.9411764705882353}, 'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.bias']}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20240161, device='cuda:0')}\n",
      "{'sigma': 2, 'scale': 1.5, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.9002557544757034, 'F1': 0.8823529411764706, 'Precision': 0.8823529411764706, 'Recall': 0.8823529411764706}, 'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.bias']}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20242962, device='cuda:0')}\n",
      "{'sigma': 2, 'scale': 2, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.9053708439897699, 'F1': 0.8823529411764706, 'Precision': 0.8823529411764706, 'Recall': 0.8823529411764706}, 'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.bias']}\n",
      "{'perZerosInModel': tensor(0.2055, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(22496536, device='cuda:0')}\n",
      "{'sigma': 3, 'scale': 1.5, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.9258312020460358, 'F1': 0.8888888888888888, 'Precision': 0.8421052631578947, 'Recall': 0.9411764705882353}, 'Pruning_layers': ['bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.bias']}\n",
      "{'perZerosInModel': tensor(0.1919, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(21008161, device='cuda:0')}\n",
      "{'sigma': 3, 'scale': 2, 'inside': True, 'RESULT': {'Accuracy': 0.9, 'ROC_AUC_score': 0.9053708439897699, 'F1': 0.8823529411764706, 'Precision': 0.8823529411764706, 'Recall': 0.8823529411764706}, 'Pruning_layers': ['bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias']}\n",
      "{'perZerosInModel': tensor(0.1849, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20243155, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    print(models[i]['data'])\n",
    "    print(models[i]['prunModel'].zeroCounter(layers)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39bc3fef-9368-4668-9326-95f1a9a721f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(0.1887, device='cuda:0'),\n tensor(0.1849, device='cuda:0'),\n tensor(0.1849, device='cuda:0'),\n tensor(0.2055, device='cuda:0'),\n tensor(0.1919, device='cuda:0'),\n tensor(0.1849, device='cuda:0')]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[models[i]['prunModel'].zeroCounter(layers)[-1]['perZerosInModel'] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cafd7d6-f3bd-42a0-99ad-4f509b928f1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5 tensor(0.7796)\n",
      "1 2 tensor(0.7689)\n",
      "2 1.5 tensor(0.9301)\n",
      "2 2 tensor(0.9342)\n",
      "3 1.5 tensor(0.9838)\n",
      "3 2 tensor(0.9827)\n"
     ]
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    layerPrun = models[i]['data']['Pruning_layers']\n",
    "    avr = [models[i]['prunModel'].zeroCounter(layerPrun)[j]['percentZero'] for j in range(len(layerPrun))]\n",
    "    print(models[i]['data']['sigma'], models[i]['data']['scale'], torch.mean(torch.Tensor(avr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d3ccd4-fad8-48ea-bb3d-e359e49bae11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigma=1:\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m     ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m2\u001B[39m]))), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m2\u001B[39m]))),\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigma=2:\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m     ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m3\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m4\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m5\u001B[39m]))), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m3\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m4\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m5\u001B[39m]))),\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigma=3:\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m----> 7\u001B[0m     ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m6\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m7\u001B[39m]) \u001B[38;5;241m^\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m8\u001B[39m]))), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m6\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m7\u001B[39m]) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(prunLayers[\u001B[38;5;241m8\u001B[39m])))\n\u001B[0;32m      8\u001B[0m )\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'sigma=1:',\n",
    "    ((len(set(prunLayers[0]) ^ set(prunLayers[1]) ^ set(prunLayers[2]))), len(set(prunLayers[0]) & set(prunLayers[1]) & set(prunLayers[2]))),\n",
    "    'sigma=2:',\n",
    "    ((len(set(prunLayers[3]) ^ set(prunLayers[4]) ^ set(prunLayers[5]))), len(set(prunLayers[3]) & set(prunLayers[4]) & set(prunLayers[5]))),\n",
    "    'sigma=3:',\n",
    "    ((len(set(prunLayers[6]) ^ set(prunLayers[7]) ^ set(prunLayers[8]))), len(set(prunLayers[6]) & set(prunLayers[7]) & set(prunLayers[8])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "738ec0c2-1af4-45e7-9ba5-6bc8b8bf82bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=1: (84, 36) scale=1.5: (94, 51) scale=2: (74, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'scale=1:',\n",
    "    ((len(set(prunLayers[0]) ^ set(prunLayers[3]) ^ set(prunLayers[6]))), len(set(prunLayers[0]) & set(prunLayers[3]) & set(prunLayers[6]))),\n",
    "    'scale=1.5:',\n",
    "    ((len(set(prunLayers[1]) ^ set(prunLayers[4]) ^ set(prunLayers[7]))), len(set(prunLayers[1]) & set(prunLayers[4]) & set(prunLayers[7]))),\n",
    "    'scale=2:',\n",
    "    ((len(set(prunLayers[2]) ^ set(prunLayers[5]) ^ set(prunLayers[8]))), len(set(prunLayers[2]) & set(prunLayers[5]) & set(prunLayers[8])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "23"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(prunLayers[0]) & set(prunLayers[3]) & set(prunLayers[6])& set(prunLayers[0]) & set(prunLayers[1]) & set(prunLayers[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(prunLayers[3]) & set(prunLayers[4]) & set(prunLayers[5]) & set(prunLayers[1]) & set(prunLayers[4]) & set(prunLayers[7]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(prunLayers[6]) & set(prunLayers[7]) & set(prunLayers[8])& set(prunLayers[2]) & set(prunLayers[5]) & set(prunLayers[8]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b73d5119-4b75-4d60-b342-67c887a9860b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PATH = \"../model-prun/models_1s_1.5-2\"\n",
    "modelsDense = torch.load(PATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5297d6bc-e046-4a3b-b4cf-0f50bedd6324",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[100, 74]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(modelsDense[i]['data']['Pruning_layers']) for i in range(2)] #199 set([1,2,3]) & set([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b3f5532-bb7f-446c-8c3d-55c6b4399505",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perZerosInModel': tensor(0.1848, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(20237128, device='cuda:0')}\n",
      "{'perZerosInModel': tensor(0.0985, device='cuda:0'), 'allParam': 109483778, 'allZero': tensor(10778722, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(modelsDense[i]['prunModel'].zeroCounter(layers)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3282d33d-dd05-4d95-97e8-ccee1180a1d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prunLayers = [modelsDense[i]['data']['Pruning_layers'] for i in range(2)] #199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5 tensor(0.7539)\n",
      "1 2 tensor(0.7555)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    layerPrun = modelsDense[i]['data']['Pruning_layers']\n",
    "    avr = [modelsDense[i]['prunModel'].zeroCounter(layerPrun)[j]['percentZero'] for j in range(len(layerPrun))]\n",
    "    print(modelsDense[i]['data']['sigma'], modelsDense[i]['data']['scale'], torch.mean(torch.Tensor(avr)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(0.1848, device='cuda:0'), tensor(0.0985, device='cuda:0')]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[modelsDense[i]['prunModel'].zeroCounter(layers)[-1]['perZerosInModel'] for i in range(2)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa1f2b9c-63da-47b4-80b9-39834fd776a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(62, 56)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(set(prunLayers[0]) ^ set(prunLayers[1])), len(set(prunLayers[0]) & set(prunLayers[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "44"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(prunLayers[0]) - set(prunLayers[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}